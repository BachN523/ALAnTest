{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Imaris2ALAn\" Data Collation Code\n",
    "# Code written by Joe Glichowski at the University of Rochester Summer 2021\n",
    "# This code will take Statistics output by Imaris Nuclear Segmentation tool and generate .csv files containing nuclear spatial coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports all necessary Python packages\n",
    "import os                                   \n",
    "import numpy as np                                     \n",
    "from scipy import optimize, stats                      \n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell specifies the directories containing \n",
    "# 1. The input: Imaris Statistics data files and \n",
    "# 2. The output: .csv files\n",
    "\n",
    "#path of input directory\n",
    "path = \"/Users/tarafinegan/Desktop/imaris/\"\n",
    "\n",
    "#define the output directory\n",
    "\n",
    "# Name of output Directory\n",
    "directory = \"ALAninput\"\n",
    "  \n",
    "# Define the location of the output Directory on your computer\n",
    "parent_dir = \"/Users/tarafinegan/Desktop/\"\n",
    "  \n",
    "# # Create the directory defined in line 11 in path defined on line 14\n",
    "outpath = os.path.join(parent_dir, directory)\n",
    "os.mkdir(outpath)\n",
    "output_path = outpath\n",
    "folder = os.listdir(path) #returns list of folders in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-07-15_07.56.05_100k_16hr_a2_Statistics',\n",
       " '2020-07-20_08.04.03_100K_8hr_a5_Statistics',\n",
       " '2020-07-15_08.47.01_200k_16hr_b4_Statistics',\n",
       " '2020-07-15_08.02.35_100k_16hr_a7_Statistics',\n",
       " '2020-07-20_08.09.15_100K_8hr_a9_Statistics',\n",
       " '2020-07-15_08.56.16_200k_16hr_b10_Statistics',\n",
       " '2020-07-15_08.41.36_200k_16hr_a16_Statistics',\n",
       " '2020-07-20_08.05.47_100K_8hr_a6_Statistics',\n",
       " '2020-07-15_08.34.25_200k_16hr_a10_Statistics',\n",
       " '2020-07-15_09.19.22_100k_16hr_b12_Statistics',\n",
       " '2020-07-20_08.01.55_100K_8hr_a4_Statistics',\n",
       " '2020-07-15_09.12.42_100k_16hr_b6_Statistics',\n",
       " '2020-07-15_08.28.47_200k_16hr_a7_Statistics',\n",
       " '.DS_Store',\n",
       " '2020-07-15_08.24.53_200k_16hr_a5_Statistics',\n",
       " '2020-07-15_08.14.58_100k_16hr_a14_Statistics',\n",
       " '2020-07-15_08.57.45_200k_16hr_b11_Statistics',\n",
       " '2020-07-15_08.22.46_200k_16hr_a4_Statistics',\n",
       " '2020-07-15_08.36.45_200k_16hr_a12_Statistics',\n",
       " '2020-07-15_09.08.09_100k_16hr_b3_Statistics',\n",
       " '2020-07-20_08.06.44_100K_8hr_a7_Statistics',\n",
       " '2020-07-15_08.29.43_200k_16hr_a8_Statistics',\n",
       " '2020-07-15_09.01.43_200k_16hr_b14_Statistics',\n",
       " '2020-07-20_07.59.49_100K_8hr_a2_Statistics',\n",
       " '2020-07-15_08.59.33_200k_16hr_b12_Statistics',\n",
       " '2020-07-15_08.13.31_100k_16hr_a13_Statistics',\n",
       " '2020-07-15_09.21.30_100k_16hr_b14_Statistics',\n",
       " '2020-07-15_08.51.59_200k_16hr_b6_Statistics',\n",
       " '2020-07-15_08.01.47_100k_16hr_a6_Statistics',\n",
       " '2020-07-15_07.57.14_100k_16hr_a4_Statistics',\n",
       " '2020-07-15_08.55.07_200k_16hr_b9_Statistics',\n",
       " '2020-07-15_08.38.05_200k_16hr_a13_Statistics',\n",
       " '2020-07-15_08.15.53_100k_16hr_a15_Statistics',\n",
       " '2020-07-15_08.52.50_200k_16hr_b7_Statistics',\n",
       " '2020-07-15_08.11.33_100k_16hr_a12_Statistics',\n",
       " '2020-07-15_09.14.33_100k_16hr_b8_Statistics',\n",
       " '2020-07-15_08.39.07_200k_16hr_a14_Statistics',\n",
       " '2020-07-15_09.00.38_200k_16hr_b13_Statistics',\n",
       " '2020-07-15_09.16.50_100k_16hr_b10_Statistics',\n",
       " '2020-07-15_08.03.34_100k_16hr_a8_Statistics',\n",
       " '2020-07-15_08.21.27_200k_16hr_a3_Statistics',\n",
       " '2020-07-15_08.43.05_200k_16hr_b1_Statistics',\n",
       " '2020-07-15_09.18.15_100k_16hr_b11_Statistics',\n",
       " '2020-07-15_09.07.03_100k_16hr_b2_Statistics',\n",
       " '2020-07-15_08.00.22_100k_16hr_a5_Statistics',\n",
       " '2020-07-15_09.04.27_200k_16hr_b16_Statistics',\n",
       " '2020-07-15_09.23.52_100k_16hr_b16_Statistics',\n",
       " '2020-07-15_08.10.15_100k_16hr_a11_Statistics',\n",
       " '2020-07-15_08.46.00_200k_16hr_b3_Statistics',\n",
       " '2020-07-15_08.33.08_200k_16hr_a9_Statistics',\n",
       " '2020-07-15_08.40.09_200k_16hr_a15_Statistics',\n",
       " '2020-07-15_09.05.54_100k_16hr_b1_Statistics',\n",
       " '2020-07-15_09.02.45_200k_16hr_b15_Statistics',\n",
       " '2020-07-15_08.05.25_100k_16hr_a9_Statistics',\n",
       " '2020-07-15_08.17.29_100k_16hr_a16_Statistics',\n",
       " '2020-07-15_09.13.38_100k_16hr_b7_Statistics',\n",
       " '2020-07-15_09.20.33_100k_16hr_b13_Statistics',\n",
       " '2020-07-15_08.27.41_200k_16hr_a6_Statistics',\n",
       " '2020-07-15_08.20.13_200k_16hr_a2_Statistics',\n",
       " '2020-07-20_08.07.41_100K_8hr_a8_Statistics',\n",
       " '2020-07-15_07.52.26_100k_16hr_a1_Statistics',\n",
       " '2020-07-15_09.15.47_100k_16hr_b9_Statistics',\n",
       " '2020-07-15_08.53.40_200k_16hr_b8_Statistics',\n",
       " '2020-07-15_07.54.27_100k_16hr_a2_Statistics',\n",
       " '2020-07-15_08.18.56_200k_16hr_a1_Statistics',\n",
       " '2020-07-15_09.09.13_100k_16hr_b4_Statistics',\n",
       " '2020-07-15_08.44.26_200k_16hr_b2_Statistics',\n",
       " '2020-07-15_08.08.08_100k_16hr_a10_Statistics',\n",
       " '2020-07-15_09.22.28_100k_16hr_b15_Statistics',\n",
       " '2020-07-15_09.11.25_100k_16hr_b5_Statistics',\n",
       " '2020-07-15_08.35.47_200k_16hr_a11_Statistics']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell writes all of the files that the code will process for the user to check\n",
    "folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell defines the function that will extract the relevant information from the Imaris statistics file and output these into a .csv that can be input into ALAn.\n",
    "\n",
    "def subfolder_name_lists(folder_address, list_of_folders, output_path):\n",
    "    \n",
    "    labels = []\n",
    "    list_of_folders.sort()\n",
    "    list_of_folders = [i for i in list_of_folders if i != '.DS_Store'] #remove unwanted element in list\n",
    "#     print(list_of_folders)\n",
    "    for item in list_of_folders:\n",
    "        x = item.split('_')\n",
    "#         print(x)\n",
    "        label = x[0] + x[1] +x[2] +x[3] #take only the desired labels for the collated file\n",
    "                                   \n",
    "#         print(str(label))\n",
    "        labels.append(str(label)) #append each label to a list for later use\n",
    "#     print(labels)\n",
    "    \n",
    "    positions = [] #list of position files names\n",
    "    volumes = [] #list of volume file names\n",
    "    subfolder_addresses = [] #list of subfolder paths\n",
    "    for item in list_of_folders:\n",
    "#         item = list_of_folders[i+1]\n",
    "        subfolder = folder_address + \"/\" + item  #subfolder address\n",
    "        subfolder_addresses.append(subfolder)\n",
    "        files = os.listdir(subfolder) #returns list of files in subfolder\n",
    "        #next two lines will assign position and volume csv file names\n",
    "        position = [f for f in files if f[-12:] == \"Position.csv\"]\n",
    "        volume = [f for f in files if f[-10:] == \"Volume.csv\"]\n",
    "        #append position/volume file names into their respective lists\n",
    "        positions.append(position)\n",
    "        volumes.append(volume)\n",
    "#         print(position,volume)\n",
    "    #next two lines transform the lists of lists of strings into lists of strings for ease of use later\n",
    "    positions = [''.join(i) for i in positions]\n",
    "    volumes = [''.join(i) for i in volumes]\n",
    "    \n",
    "    #need to go into folder then subfolder then pd.read_excel the desired csv, then append to a list of dfs\n",
    "    #use list of subfolder addresses to bypass first step, then just say for each subfolder address add on our\n",
    "    #position of volume files suffix and pd.read_excel that sucker\n",
    "    position_dfs = []\n",
    "    volume_dfs = []\n",
    "    \n",
    "    for i in range(len(subfolder_addresses)):\n",
    "        \n",
    "        #read in position csv's as data frames and append to list\n",
    "        position_address = subfolder_addresses[i] + \"/\" + positions[i]\n",
    "        df = pd.read_csv(position_address, encoding = \"utf-8\", skiprows = 3)\n",
    "        df = df.drop(['Unit', \"Category\", \"Collection\", \"Time\", \"CellID\", \"ID\", \"Unnamed: 9\"], axis = 1) #dropping unwanted columns\n",
    "        position_dfs.append(df)\n",
    "        \n",
    "        #read in volume csv's as data frames and append to list\n",
    "        volume_address = subfolder_addresses[i] + \"/\" + volumes[i]\n",
    "        volume_dfs.append(pd.read_csv(volume_address, encoding = \"utf-8\", skiprows = 3))\n",
    "    \n",
    "    #taking the one column we want from the volume csv's and merging it with the processed position csv's\n",
    "    merged_dfs = []\n",
    "    for i in range(len(volume_dfs)):\n",
    "        df = volume_dfs[i]\n",
    "        sub_df = df[\"Nucleus Volume\"] #single column we want\n",
    "        p_df = position_dfs[i]\n",
    "        merged_df = pd.concat([p_df, sub_df], axis=1)\n",
    "        merged_dfs.append(merged_df)\n",
    "        #saves collated data as csv's in desired location\n",
    "        merged_df.to_csv(output_path + '/' + labels[i] + '.csv', index = False)\n",
    "        \n",
    "    return merged_dfs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3273320198059082\n"
     ]
    }
   ],
   "source": [
    "# This cell will run the function on your files and time taken for this to occur, outputting the time in seconds for this to occur.\n",
    "\n",
    "toc = time.time()\n",
    "subfolder_name_lists(path, folder, output_path)\n",
    "tic = time.time()\n",
    "print(tic-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
