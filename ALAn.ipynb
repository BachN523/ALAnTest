{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following functions make up the Automated Layer Analysis tool. Individually, functions extract information from image and segmented nuclei data to assess layer properties as described in the read-me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                   \n",
    "import numpy as np                            \n",
    "import skimage.io as io            \n",
    "from scipy import optimize, stats                      \n",
    "import pandas as pd                             \n",
    "import math\n",
    "import inspect\n",
    "import time\n",
    "from scipy import signal\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt                   \n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This first cell is intended to read in your data set as quickly and hassle free as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6281142234802246\n"
     ]
    }
   ],
   "source": [
    "toc = time.time()\n",
    "\n",
    "#import and read raw data as data files and images from designated directory\n",
    "\n",
    "path = \"your path\"\n",
    "files = os.listdir(path) #returns list of files in directory\n",
    "#Sorts excel files and images into their own lists\n",
    "excel_files = [f for f in files if f[-4:] == 'xlsx']\n",
    "images = [f for f in files if f[-3:] == 'tif']\n",
    "    \n",
    "#Makes a separate list of the data files with each variable name converted to a string (list of strings)\n",
    "#Additionally, each variable has the \".tif\" removed from its end\n",
    "list_of_image_names = [sub[:-4] for sub in images]\n",
    "\n",
    "#Sorts lists of excel and image files in the same manner/method dicatated by the sort function.\n",
    "#Now, the two lists should line up excatly, i.e. first element of each correspond to the same test/well,\n",
    "# howoever, due to unpaired images/sheets (e.g and excel file with no corresponding image) \n",
    "# we must first remove said unpaired elements before the two lists will align\n",
    "excel_files.sort()\n",
    "images.sort()\n",
    "\n",
    "#Want to eliminate elements which do not have a corresponding image/excel sheet.\n",
    "#Already have list_of_image_names â€” can also make list_of_excel_names then use splicing to cross reference.\n",
    "list_of_excel_names = [sub[:-5] for sub in excel_files] #cuts \".xlsx\" off the end of the list of excel files\n",
    "\n",
    "#Defines two list which consist of elements not present in both lists\n",
    "unmatched_excels = [i for i in list_of_excel_names if i[:] not in list_of_image_names]\n",
    "unmatched_images = [i for i in list_of_image_names if i[:] not in list_of_excel_names]\n",
    "\n",
    "#Removes the unmatched excel list elements, adds the \".xlsx\" suffix onto each element of the list, and then defines the\n",
    "# list with the suffix as a new, aligned list\n",
    "if len(unmatched_excels) > 0:  \n",
    "    for i in range(len(unmatched_excels)):\n",
    "        list_of_excel_names.remove(unmatched_excels[i]) \n",
    "excel_suffix = '.xlsx'\n",
    "aligned_excel_files = [sub + excel_suffix for sub in list_of_excel_names]\n",
    "aligned_excel_files.sort()\n",
    "#Removes the unmatched image list elements, adds the \".tif\" suffix onto each element of the list, and then defines the\n",
    "# list with the suffix as a new, aligned list\n",
    "if len(unmatched_images) > 0:\n",
    "    for i in range(len(unmatched_images)):\n",
    "        list_of_image_names.remove(unmatched_images[i])\n",
    "image_suffix = '.tif'\n",
    "aligned_images = [sub + image_suffix for sub in list_of_image_names]\n",
    "aligned_images.sort()\n",
    "#Makes a list of the excel files read through as data files (list of variables)\n",
    "list_of_dfs = []\n",
    "for item in aligned_excel_files:\n",
    "    list_of_dfs.append(pd.read_excel(path + item))\n",
    "    \n",
    "#Makes a list of read-in images which still need to be systemized using image shuffle\n",
    "list_of_unshuffled_images = []\n",
    "\n",
    "for item in aligned_images:\n",
    "    list_of_unshuffled_images.append(io.imread(path + item))\n",
    "\n",
    "tic = time.time()\n",
    "print(tic-toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_images = []\n",
    "for image in list_of_unshuffled_images:\n",
    "    list_of_images.append(image_shuffle(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following functions are detailed in the read me, along with example uses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003838777542114258\n"
     ]
    }
   ],
   "source": [
    "toc = time.time()\n",
    "\n",
    "### image shuffle ensures all of the images have the channels and xyz coordinates in the same order\n",
    "### and makes a new list of images so the following definitions work correctly\n",
    "\n",
    "#Since we know that x and y must each be 512 in length for our images, each logic statement starts by assigning\n",
    "#the x and y axis to those variables (a,b,c,d) which have value 512. Then, we use the criteria that there can only\n",
    "#be 4 colors maximum in our image. Therefore, after assigning the x and y axis, a variable with value \n",
    "#greater than 4 must represent the z-axis, and the remainder the color.\n",
    "## In order to use this routine for an image of another size, one only has to change the length (512) values within the \n",
    "## routine to the desired length. Otherwise, this program will not work as intended.\n",
    "def image_shuffle(image):\n",
    "    a, b, c, d = image.shape\n",
    "    if a == 512:\n",
    "        xloc = 0\n",
    "        if b == 512:\n",
    "            yloc = 1\n",
    "            if c >4:\n",
    "                zloc = 2\n",
    "                colloc = 3\n",
    "            else:\n",
    "                zloc = 3\n",
    "                colloc = 2\n",
    "        elif c == 512:\n",
    "            yloc = 2\n",
    "            if b > 4:\n",
    "                zloc = 1\n",
    "                colloc = 3\n",
    "            else:\n",
    "                zloc = 3\n",
    "                colloc = 1\n",
    "        else:\n",
    "            yloc = 3\n",
    "            if b > 4:\n",
    "                zloc = 1\n",
    "                colloc = 2\n",
    "            else:\n",
    "                zloc = 2\n",
    "                colloc = 1\n",
    "    elif b == 512:\n",
    "        xloc = 1\n",
    "        if c == 512:\n",
    "            yloc = 2\n",
    "            if a > 4:\n",
    "                zloc = 0\n",
    "                colloc = 3\n",
    "            else:\n",
    "                zloc = 3\n",
    "                colloc = 0\n",
    "        else:\n",
    "            yloc = 3\n",
    "            if a > 4:\n",
    "                zloc = 0\n",
    "                colloc = 2\n",
    "            else:\n",
    "                zloc = 2\n",
    "                colloc = 0\n",
    "    else:\n",
    "        xloc = 2\n",
    "        yloc = 3\n",
    "        if a > 4:\n",
    "            zloc = 0\n",
    "            colloc = 1\n",
    "        else:\n",
    "            zloc = 1\n",
    "            colloc = 0\n",
    "    return np.transpose(image, (zloc, colloc, xloc, yloc))\n",
    "\n",
    "### Get_layer_position extracts the x, y, z min and max positions from the microscope, returns them, and \n",
    "### also defines an array of equal value slices which add up to form the entire layer.\n",
    "    \n",
    "def get_layer_position(df, image):\n",
    "    num_z, num_c, num_x, num_y = image.shape\n",
    "    df.columns = ['x', 'y', 'z', 'vol', 'positions']\n",
    "    x_max = df['positions'][0]\n",
    "    y_max = df['positions'][1]\n",
    "    z_max = df['positions'][2]\n",
    "    x_min = df['positions'][3]\n",
    "    y_min = df['positions'][4]\n",
    "    z_min = df['positions'][5]\n",
    "    slice_heights = np.linspace(0, z_max-z_min, num = num_z)\n",
    "    return (x_max, y_max, z_max, x_min, y_min, z_min, slice_heights)\n",
    "\n",
    "### local density allows a user to define a smaller area (a box) to analyze if desired. The default is the maximum size\n",
    "### of the image provided\n",
    "def local_density(df, image, **kwargs):\n",
    "    x_max, y_max, z_max, x_min, y_min, z_min, slice_heights = get_layer_position(df, image)\n",
    "    box_radius = kwargs.get('box_radius', (x_max-x_min)/2) #assigning radius value according to desired box size\n",
    "    df_cleared = clear_debris(df) #clearing out background noise\n",
    "    xcent= (xmin+xmax)/2 \n",
    "    ycent= (ymin+ymax)/2 #defining centers of the x and y planes of layer\n",
    "    xmin_box, xmax_box = xcent - box_radius, xcent + box_radius\n",
    "    ymin_box, ymax_box = ycent- box_radius, ycent + box_radius #defining bounds in x & y for the chosen box\n",
    "    df_box = df_cleared[(df_cleared['x'].values >= xmin_box)& \n",
    "                        (df_cleared['x'].values <= xmax_box)&\n",
    "                        (df_cleared['y'].values >= ymin_box)&\n",
    "                        (df_cleared['y'].values <= ymax_box)] #Clearing only those cells within the given box parameters\n",
    "    number_of_cells_in_box = len(df_box) #number of cells in box is the same as the number cleared in the previous line\n",
    "    box_density = (number_of_cells_in_box / (4*box_radius**2))*1000\n",
    "    return number_of_cells_in_box, box_density\n",
    "\n",
    "\n",
    "### clear debris removes any volumes in the provided segmentation file to ensure that only nuclei are being \n",
    "### included in the following analysis\n",
    "\n",
    "#Removes false nuclei/background noise based upon the volume of the sources. \n",
    "def clear_debris(df, **kwargs):\n",
    "    df.columns = ['x', 'y', 'z', 'vol', 'positions']\n",
    "    plot = kwargs.get('plot', False)\n",
    "    save_name = kwargs.get('save_name', False)\n",
    "    vols = df['vol'].values \n",
    "    bottom_vol = np.mean(vols)- (1.5*np.std(vols)) #cutoff volume\n",
    "    df_cleared = df[df['vol']>=bottom_vol] #all sources below the cutoff volume are excluded\n",
    "    if plot == True: #If plot is passed as an argument, the following plotting sequence will run.\n",
    "        bins = np.linspace(np.min(vols), np.max(vols), num = 100)\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.hist(vols, bins = bins, alpha = 1, label= 'Debris')\n",
    "        ax.hist(df_cleared['vol'].values, bins = bins, alpha = 1, label = 'Cells')\n",
    "        ax.legend(loc = 'upper right')\n",
    "        ax.set_ylabel('Number of cells')\n",
    "        ax.set_xlabel('Nucleus volume ($\\mu$m$^3$)')\n",
    "        ax.axvline(x = bottom_vol, c='k', linestyle = '--')\n",
    "        fig.show\n",
    "        if save_name != False: #If a save_name is passed as an argument, the figure will be saved. This idea holds for all \n",
    "                                #future routines where this is used.\n",
    "            plt.savefig(save_name + '.pdf', bbox_inches = 'tight', pad_inches = 1)\n",
    "    return(df_cleared)\n",
    "\n",
    "### layer height actin uses the xy projection of the image in order \n",
    "#Uses the normalized actin intensity graph to determine z positions for the start/end of a given layer. Also \n",
    "#determines layer height and the actin peak location\n",
    "def layer_height_actin(df, image, **kwargs):\n",
    "    x_max, y_max, z_max, x_min, y_min, z_min, slice_heights = get_layer_position(df, image)\n",
    "    plot = kwargs.get('plot', False)\n",
    "    save_name = kwargs.get('save_name', False)\n",
    "    actin_channel = kwargs.get('actin_channel', 1)\n",
    "    \n",
    "    \n",
    "    xy_proj_actin = np.sum(np.sum(image, axis = 3), axis = 2)[:,actin_channel].copy() #measuring the xy projecting of the actin intensity\n",
    "                                                                                      # x and y were assigned to axis 2 and 3 in image shuffle\n",
    "    norm_intensities_actin = (xy_proj_actin-np.min(xy_proj_actin))/np.max(xy_proj_actin-np.min(xy_proj_actin)) #normalize actin graph\n",
    "    \n",
    "    actin_peak = slice_heights[np.argwhere(norm_intensities_actin == np.max(norm_intensities_actin))][0][0] #determine actin peak z-value\n",
    "    df_cleared = clear_debris(df)\n",
    "    density = len(df_cleared)/(x_max-x_min)**2*1000\n",
    "    if density <=2:\n",
    "        bot_cutoff = 0.5\n",
    "        top_cutoff = 0.6\n",
    "    elif density>=6:\n",
    "        bot_cutoff = 0.2\n",
    "        top_cutoff = 0.8\n",
    "    else:\n",
    "        bot_cutoff = 0.5-0.3*(density-2)/4\n",
    "        top_cutoff = 0.6+0.2*(density-2)/4\n",
    "    print(density, bot_cutoff, top_cutoff)\n",
    "    min_actin_slice = np.argwhere(norm_intensities_actin >= bot_cutoff)[0][0]\n",
    "    max_actin_slice = np.argwhere(norm_intensities_actin >= top_cutoff)[-1][0] #Determining locations of bottom and top of layer using empirical results\n",
    "    min_layer_height = slice_heights[min_actin_slice]\n",
    "    max_layer_height = slice_heights[max_actin_slice] #Determining values of layer height at bottom and top bounds found above\n",
    "    layer_height = max_layer_height-min_layer_height\n",
    "    actin_intensity_top = norm_intensities_actin[max_actin_slice] #actin intensity value at top of layer\n",
    "    \n",
    "    if (plot == True):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(slice_heights, norm_intensities_actin, c='cornflowerblue')\n",
    "        ax.set_ylabel('Actin Intensity (A.U.)')\n",
    "        ax.set_xlabel('Z-Position ($\\mu$m)')\n",
    "        ax.axvline(x = min_layer_height, c='k', linestyle = '--', label = 'Layer Bounds')\n",
    "        ax.axvline(x = max_layer_height, c = 'k', linestyle = '--')\n",
    "        ax.axvline(x = np.max(slice_heights), c = 'r', linestyle = '--')\n",
    "        ax.legend(loc='upper right')\n",
    "        ax.set_ylim([0,1.1])\n",
    "        ax.set_xlim([0, 30])\n",
    "        if save_name != False:\n",
    "            plt.savefig(save_name + '.pdf', bbox_inches = 'tight', pad_inches = 1)\n",
    "    return (min_layer_height, max_layer_height, layer_height, actin_peak, norm_intensities_actin)\n",
    "\n",
    "#Smooths actin intensity graph so that it can be differentiated nicely. This is done so that when we use the derivative\n",
    "# in the next routine, find_shoulders, there are no problems due to the jagged areas of original actin intensity graph.\n",
    "## One thing to note is that the smoothed graphs for both the mature and immature cases are normalized, so they appear\n",
    "## to have the same intensity even though the mature case has a higher absolute intensity.\n",
    "def smooth_array(array, **kwargs):\n",
    "    number_to_smooth = kwargs.get('window_size', 3)\n",
    "    window = np.ones(number_to_smooth)/number_to_smooth #array to use in convolution\n",
    "    new_array = np.convolve(array, window, mode='same') #Convoluting a given array in order to smooth sharp edges\n",
    "    return new_array\n",
    "\n",
    "#We can differentiate between mature and immature based upon the presence of a secondary, \"shoulder\" on the actin \n",
    "#intensity graph. Mature cases will have the shoulder which is a result of the increased presence of lateral (cell to \n",
    "#cell) actin relative to immature cases. The following routine detects the presence of shoulders which have a prominence\n",
    "#above a certain degree. It does this using the derivative and a given prominence value.\n",
    "def find_shoulders(df, image, **kwargs):\n",
    "    x_max, y_max, z_max, x_min, y_min, z_min, slice_heights = get_layer_position(df, image)\n",
    "    plot = kwargs.get('plot', False)\n",
    "    save_name = kwargs.get('save_name', False)\n",
    "    actin_channel  = kwargs.get('actin_channel', 1)\n",
    "    xy_proj_actin = np.sum(np.sum(image, axis = 3), axis = 2)[:,actin_channel].copy()\n",
    "    norm_intensities_actin = (xy_proj_actin-np.min(xy_proj_actin))/np.max(xy_proj_actin-np.min(xy_proj_actin))\n",
    "    actin_profile_derivative = smooth_array(np.diff(norm_intensities_actin), window_size = 5)\n",
    "    peaks = signal.find_peaks(actin_profile_derivative, prominence = 0.008)\n",
    "    \n",
    "    if len(peaks[0])==2:\n",
    "        equivalence = peaks[1]['prominences'][1]/peaks[1]['prominences'][0]\n",
    "    else:\n",
    "        equivalence = 'undef'\n",
    "    \n",
    "    if plot == True or save_name != False:\n",
    "        fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (7,7))\n",
    "        ax.plot(slice_heights, norm_intensities_actin, 'cornflowerblue', label = 'Actin Profile')\n",
    "        ax.set_ylabel('Actin Intensity (A.U.)')\n",
    "        ax.set_xlabel('Z-Position ($\\mu$m)')\n",
    "        ax2=ax.twinx()\n",
    "        ax2.plot(slice_heights[:-1] + (z_max-z_min)/(2*len(slice_heights)), actin_profile_derivative, 'steelblue', label = 'Actin Derivative')\n",
    "        ax2.set_xlabel('Z-Position ($\\mu$m)')\n",
    "        ax2.set_ylabel('Actin Profile 1$^{st}$ Derivative (A.U. / $\\mu$m)')\n",
    "        fig.tight_layout()\n",
    "        fig.show()\n",
    "        if save_name != False:\n",
    "            plt.savefig(save_name + '.pdf', bbox_inches = 'tight', pad_inches = 1)\n",
    "    \n",
    "    return (equivalence, len(peaks[0]), peaks[1]['prominences'][1], peaks[1]['prominences'][0])\n",
    "\n",
    "def gaussian_fit(x, A, B, C):\n",
    "    return A * np.exp(-1 * ((x - B) / C) ** 2)\n",
    "\n",
    "def double_gaussian_fit(x, A, B, C, D, E, F):\n",
    "    return A * np.exp(-1 * ((x - B) / C) ** 2) + D * np.exp(-1 * ((x - E) / F) ** 2)\n",
    "\n",
    "#Classifies layers as organized, disorganized, or undefined as well as finds the location of the nuclear peak\n",
    "def nuclei_distribution(df, image, **kwargs):\n",
    "    x_max, y_max, z_max, x_min, y_min, z_min, slice_heights = get_layer_position(df, image)\n",
    "    min_layer_height, max_layer_height, layer_height, actin_peak, norm_intensities_actin = layer_height_actin(df, image)\n",
    "    plot = kwargs.get('plot', False)\n",
    "    save_name = kwargs.get('save_name', False)\n",
    "    df_cleared = clear_debris(df)\n",
    "    zs = df_cleared['z'].values - z_min\n",
    "    bins = np.linspace(0, 50, 51)\n",
    "    bins_fit = np.linspace(0, 50, 1000)\n",
    "    counts, bins_aux = np.histogram(zs, bins = bins)\n",
    "    counts_to_fit = smooth_array(counts)\n",
    "    p0_double = [np.max(counts_to_fit), 8, 2, 30, 15, 5]\n",
    "    p0_single = [np.max(counts_to_fit), 8, 2]\n",
    "    bounds_double = ([0, 1, 1, 0, 1, 1], [400, 80, 15, 400, 80, 15])\n",
    "    bounds_single = ([0, 1, 1], [400, 80, 15])\n",
    "    peak_loc = np.argwhere(counts_to_fit==np.max(counts_to_fit))[0][0]\n",
    "    params_double, params_covariance_double = optimize.curve_fit(double_gaussian_fit, bins[:-1], counts_to_fit, p0_double, bounds = bounds_double)\n",
    "    params_single, params_covariance_single = optimize.curve_fit(gaussian_fit, bins[:-1], counts_to_fit, p0_single, bounds = bounds_single)\n",
    "    fit_single = gaussian_fit(bins_fit, params_single[0], params_single[1], params_single[2])\n",
    "    fit_double = double_gaussian_fit(bins_fit, params_double[0], params_double[1], params_double[2], params_double[3], params_double[4], params_double[5])\n",
    "    \n",
    "    mean_deviation = np.sqrt(np.mean((fit_single - fit_double)**2))\n",
    "    \n",
    "    if mean_deviation >5:\n",
    "        peaks = 2\n",
    "        if params_double[0]>=params_double[3]:\n",
    "            tall_peak = params_double[:3]\n",
    "            short_peak = params_double[3:]\n",
    "        else:\n",
    "            tall_peak = params_double[3:] \n",
    "            short_peak = params_double[:3]\n",
    "        \n",
    "        if params_double[1]<=params_double[4]:\n",
    "            left_peak = params_double[:3]\n",
    "            right_peak = params_double[3:]\n",
    "        else:\n",
    "            left_peak = params_double[3:] \n",
    "            right_peak = params_double[:3]\n",
    "        \n",
    "        a1, b1, c1 = left_peak\n",
    "        a2, b2, c2 = right_peak\n",
    "    \n",
    "        A = 1/c2**2-1/c1**2\n",
    "        B = 2*b1/c1**2-2*b2/c2**2\n",
    "        C = (b2/c2)**2-(b1/c1)**2+np.log(a1/a2)\n",
    "    \n",
    "        roots = np.roots([A,B,C])\n",
    "        intersection = np.max(gaussian_fit(roots, a1,b1,c1))\n",
    "        same_spot_value = intersection / short_peak[0]\n",
    "\n",
    "        if same_spot_value <= 0.5:\n",
    "            layer = 'organized'\n",
    "            nuclear_peak = left_peak[1]\n",
    "        else:\n",
    "            layer = 'disorganized'\n",
    "            nuclear_peak = left_peak[1]\n",
    "    else:\n",
    "        peaks = 1\n",
    "        same_spot_value = 'undef'\n",
    "        if params_single[2] > -0.372*(params_single[1]-min_layer_height) + 3.572:\n",
    "            layer = 'disorganized'\n",
    "            nuclear_peak = params_single[1]\n",
    "        else:\n",
    "            layer = 'organized'\n",
    "            nuclear_peak = params_single[1]\n",
    "            \n",
    "\n",
    "\n",
    "    if (plot == True or save_name != False):\n",
    "        fig, ax = plt.subplots()#(figsize =(3,2))\n",
    "        ax.scatter(bins[:-1], counts_to_fit, label = 'Nuclear Position')\n",
    "        ax.plot(bins_fit, fit_double, label = 'Double Gaussian Fit')\n",
    "        ax.set_ylabel('Number of nuclei')\n",
    "        ax.set_xlabel('Z Position ($\\mu$m)')\n",
    "        ax.set_xlim(0, 30)\n",
    "        ax2=ax.twinx()\n",
    "        ax2.plot(slice_heights, norm_intensities_actin, c='r')\n",
    "        ax2.set_ylabel('Actin Intensity (A.U.)')\n",
    "        ax2.axvline(x = min_layer_height, c='k', linestyle = '--', label = 'Layer Bounds')\n",
    "        ax2.axvline(x = max_layer_height, c = 'k', linestyle = '--')\n",
    "        print(min_layer_height, max_layer_height)#, params_single[1]-min_layer_height, params_single[2])\n",
    "        fig.show\n",
    "        if save_name != False:\n",
    "            plt.savefig(save_name + '.pdf', bbox_inches = 'tight', pad_inches = 1)\n",
    "            \n",
    "    \n",
    "    return nuclear_peak, layer\n",
    "\n",
    "#Plots a scatter plot of terrain in the z-axis (top-down view).\n",
    "##Size of each plotted circle is directly proportional to the volume of the cell it represents\n",
    "###Darker color represents higher z value\n",
    "def terrain_map(df, image, **kwargs):\n",
    "    color = kwargs.get('color', 'magma_r')\n",
    "    x_max, y_max, z_max, x_min, y_min, z_min, slice_heights = get_layer_position(df, image)\n",
    "    df_cleared = clear_debris(df)\n",
    "    save_name = kwargs.get('save_name', False)\n",
    "    xs = df_cleared['x'].values - x_min\n",
    "    ys = df_cleared['y'].values - y_min\n",
    "    zs = df_cleared['z'].values - z_min\n",
    "    vols = df_cleared['vol'].values\n",
    "    cross_sections = np.pi*(np.cbrt(vols*3/(4*np.pi)))**2 #calculates size of circle from greatest cross section of cell\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    ax.scatter(xs, -ys, s = cross_sections/2, c = zs, cmap = color, vmin = 0, vmax = 30)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlim([0, x_max-x_min])\n",
    "    ax.set_ylim([-(y_max-y_min), 0])\n",
    "    if save_name != False:\n",
    "        plt.savefig(save_name + '.pdf', bbox_inches = 'tight', pad_inches = 1)\n",
    "    return\n",
    "\n",
    "#Determines number of cells both above and inside the layer, and from this the percent of cells above the layer.\n",
    "##Also returns cell density and layer classified based upon given criteria\n",
    "def layer_determination(df, image, **kwargs):\n",
    "    df_cleared = clear_debris(df)\n",
    "    x_max, y_max, z_max, x_min, y_min, z_min, slice_heights = get_layer_position(df, image)\n",
    "    min_layer_height, max_layer_height, layer_height, actin_peak, norm_intensities_actin = layer_height_actin(df, image)\n",
    "    equivalence, num_peaks, p1, p2 = find_shoulders(df, image)\n",
    "    nuclear_peak, layer = nuclei_distribution(df, image)\n",
    "    \n",
    "    actin_to_top = max_layer_height - actin_peak\n",
    "    actin_to_bottom = actin_peak - min_layer_height\n",
    "\n",
    "\n",
    "    peak_difference_rule = (actin_peak - nuclear_peak)>= 0\n",
    "    actin_peak_position_rule = (actin_to_top - actin_to_bottom)<0\n",
    "    deriv_peak_rule = num_peaks == 1 \n",
    "    \n",
    "    cells_above = np.sum(df_cleared['z']-z_min >= max_layer_height)\n",
    "    cells_inside = np.sum(df_cleared['z']-z_min < max_layer_height)\n",
    "    percentage_above = (cells_above/(cells_above+cells_inside)*100)\n",
    "    cell_density = (cells_inside+cells_above) / (x_max-x_min)**2*1000\n",
    "    \n",
    "\n",
    "    if layer == 'disorganized':\n",
    "        layer_classification = 'Disorganized'\n",
    "        \n",
    "    else:\n",
    "        if actin_peak_position_rule and peak_difference_rule:\n",
    "            if deriv_peak_rule or equivalence < 1:\n",
    "                layer_classification = 'Intermediate' \n",
    "            else:\n",
    "                layer_classification = 'Mature'\n",
    "        else:\n",
    "            layer_classification = 'Immature'\n",
    "   \n",
    "    return layer_classification, cells_above, cells_inside, percentage_above, cell_density\n",
    "\n",
    "#Determines number of cells within the confines of the layer\n",
    "def disorganization_extent(df, image, **kwargs):\n",
    "    plot = kwargs.get('plot', False)\n",
    "    save_name = kwargs.get('save_name', False)\n",
    "    actin_channel = kwargs.get('actin_channel', 0)\n",
    "    x_max, y_max, z_max, x_min, y_min, z_min, slice_heights = get_layer_position(df, image)\n",
    "    df_cleared = clear_debris(df)\n",
    "    min_layer_height, max_layer_height, layer_height, actin_peak, norm_intensities_actin = layer_height_actin(df, image)\n",
    "    \n",
    "    density = len(df_cleared)/(x_max-x_min)**2*1000\n",
    "    if density <=2:\n",
    "        bot_cutoff = 0.5\n",
    "        top_cutoff = 0.6\n",
    "    elif density>=6:\n",
    "        bot_cutoff = 0.2\n",
    "        top_cutoff = 0.8\n",
    "    else:\n",
    "        bot_cutoff = 0.5-0.3*(density-2)/4\n",
    "        top_cutoff = 0.6+0.2*(density-2)/4\n",
    "    print(density, bot_cutoff, top_cutoff)\n",
    "    min_actin_slice = np.argwhere(norm_intensities_actin >= bot_cutoff)[0][0]\n",
    "    max_actin_slice = min_actin_slice+15\n",
    "    \n",
    "    min_slice = slice_heights[min_actin_slice]\n",
    "    max_slice = slice_heights[min_actin_slice]+8\n",
    "    sum_proj_z = np.sum(image[min_actin_slice:max_actin_slice, actin_channel, :, :], axis = 0)\n",
    "    df_clear_in_layer = df_cleared[df_cleared['z']>(min_slice+z_min)]\n",
    "    df_clear_in_layer = df_clear_in_layer[df_clear_in_layer['z']<(max_slice+z_min)]\n",
    "    real_xs = (df_clear_in_layer['x'] - x_min)*sum_proj_z.shape[0]/(x_max-x_min)\n",
    "    real_ys = (df_clear_in_layer['y'] - y_min)*sum_proj_z.shape[1]/(y_max-y_min)\n",
    "    \n",
    "    basal_cells = len(df_clear_in_layer)\n",
    "    \n",
    "    if (plot == True):\n",
    "        fig, ax = plt.subplots(figsize = (7,7))\n",
    "        ax.imshow(sum_proj_z, cmap = 'Greys')\n",
    "        ax.scatter(real_xs, real_ys, c = 'k')\n",
    "        ax.set_xlim([0, sum_proj_z.shape[0]])\n",
    "        ax.set_ylim([0, sum_proj_z.shape[1]])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if save_name != False:\n",
    "            plt.savefig(save_name + '.pdf', bbox_inches = 'tight', pad_inches = 1)\n",
    "    \n",
    "    return(basal_cells)\n",
    "\n",
    "\n",
    "def batch_process(list_of_dfs, list_of_unshuffled_images, list_of_names, **kwargs):\n",
    "    box_radius = kwargs.get('box_radius', False)\n",
    "    save_name = kwargs.get('save_name', False)\n",
    "    list_of_cells_above = []\n",
    "    list_of_percent_above = []\n",
    "    list_of_cells_in = []\n",
    "    list_of_layer_class = []\n",
    "    list_of_cell_densities = []\n",
    "    list_of_layer_heights = []\n",
    "    list_of_bad_images = []\n",
    "    list_of_disorganization_extent = []\n",
    "    list_of_equivalences = []\n",
    "    list_of_widths = []\n",
    "    list_of_peak_positions = []\n",
    "    list_of_peak_numbers = []\n",
    "\n",
    "    if box_radius != False:\n",
    "        list_of_box_cells = []\n",
    "        list_of_box_densities = []\n",
    "\n",
    "    for i in range(len(list_of_dfs)):\n",
    "        try:\n",
    "            df = list_of_dfs[i]\n",
    "            image = image_shuffle(list_of_unshuffled_images[i])\n",
    "                  \n",
    "            # Defining get_layer_position routine variables:\n",
    "            num_z, num_c, num_x, num_y = image.shape\n",
    "            df.columns = ['x', 'y', 'z', 'vol', 'positions']\n",
    "            x_max = df['positions'][0]\n",
    "            y_max = df['positions'][1]\n",
    "            z_max = df['positions'][2]\n",
    "            x_min = df['positions'][3]\n",
    "            y_min = df['positions'][4]\n",
    "            z_min = df['positions'][5]\n",
    "            slice_heights = np.linspace(0, z_max-z_min, num = num_z)\n",
    "\n",
    "            # Portion of clear_debris which calculates value of df_cleared\n",
    "            vols = df['vol'].values \n",
    "            bottom_vol = np.mean(vols)- (1.5*np.std(vols)) #cutoff volume\n",
    "            df_cleared = df[df['vol']>=bottom_vol] #all sources below the cutoff volume are excluded\n",
    "            \n",
    "            # Defining desired variables from layer_height_actin routine\n",
    "            actin_channel = kwargs.get('actin_channel', 1)\n",
    "            xy_proj_actin = np.sum(np.sum(image, axis = 3), axis = 2)[:,actin_channel].copy() #measuring the xy projecting of the actin intensity\n",
    "                                                                                              # x and y were assigned to axis 2 and 3 in image shuffle\n",
    "            norm_intensities_actin = (xy_proj_actin-np.min(xy_proj_actin))/np.max(xy_proj_actin-np.min(xy_proj_actin)) #normalize actin graph\n",
    "            actin_peak = slice_heights[np.argwhere(norm_intensities_actin == np.max(norm_intensities_actin))][0][0] #determine actin peak z-value\n",
    "            density = len(df_cleared)/(x_max-x_min)**2*1000\n",
    "            if density <=2:\n",
    "                bot_cutoff = 0.5\n",
    "                top_cutoff = 0.6\n",
    "            elif density>=6:\n",
    "                bot_cutoff = 0.2\n",
    "                top_cutoff = 0.8\n",
    "            else:\n",
    "                bot_cutoff = 0.5-0.3*(density-2)/4\n",
    "                top_cutoff = 0.6+0.2*(density-2)/4\n",
    "            min_actin_slice = np.argwhere(norm_intensities_actin >= bot_cutoff)[0][0]\n",
    "            max_actin_slice = np.argwhere(norm_intensities_actin >= top_cutoff)[-1][0] #Determining locations of bottom and top of layer using empirical results\n",
    "            min_layer_height = slice_heights[min_actin_slice]\n",
    "            max_layer_height = slice_heights[max_actin_slice] #Determining values of layer height at bottom and top bounds found above\n",
    "            layer_height = max_layer_height-min_layer_height\n",
    "            actin_intensity_top = norm_intensities_actin[max_actin_slice] #actin intensity value at top of layer\n",
    "\n",
    "            # Defining nuclei_distribution variables\n",
    "            zs = df_cleared['z'].values - z_min\n",
    "            bins = np.linspace(0, 50, 51)\n",
    "            bins_fit = np.linspace(0, 50, 1000)\n",
    "            counts, bins_aux = np.histogram(zs, bins = bins)\n",
    "            counts_to_fit = smooth_array(counts)\n",
    "            peak_height_guess = np.max(counts_to_fit)\n",
    "            peak_loc_guess = bins[np.argwhere(counts_to_fit==peak_height_guess)[0][0]]\n",
    "            p0_double = [peak_height_guess, peak_loc_guess, 2, 30, 2*peak_loc_guess, 5]\n",
    "            p0_single = [peak_height_guess, peak_loc_guess, 2]\n",
    "            bounds_double = ([0, 1, 1, 0, 1, 1], [400, 80, 15, 400, 80, 15])\n",
    "            bounds_single = ([0, 1, 1], [400, 80, 15])\n",
    "            peak_loc = np.argwhere(counts_to_fit==np.max(counts_to_fit))[0][0]\n",
    "            params_double, params_covariance_double = optimize.curve_fit(double_gaussian_fit, bins[:-1], counts_to_fit, p0_double, bounds = bounds_double)\n",
    "            params_single, params_covariance_single = optimize.curve_fit(gaussian_fit, bins[:-1], counts_to_fit, p0_single, bounds = bounds_single)\n",
    "            fit_single = gaussian_fit(bins_fit, params_single[0], params_single[1], params_single[2])\n",
    "            fit_double = double_gaussian_fit(bins_fit, params_double[0], params_double[1], params_double[2], params_double[3], params_double[4], params_double[5])\n",
    "\n",
    "            mean_deviation = np.sqrt(np.mean((fit_single - fit_double)**2))\n",
    "\n",
    "            if mean_deviation >5:\n",
    "                peaks = 2\n",
    "                if params_double[0]>=params_double[3]:\n",
    "                    tall_peak = params_double[:3]\n",
    "                    short_peak = params_double[3:]\n",
    "                else:\n",
    "                    tall_peak = params_double[3:] \n",
    "                    short_peak = params_double[:3]\n",
    "\n",
    "                if params_double[1]<=params_double[4]:\n",
    "                    left_peak = params_double[:3]\n",
    "                    right_peak = params_double[3:]\n",
    "                else:\n",
    "                    left_peak = params_double[3:] \n",
    "                    right_peak = params_double[:3]\n",
    "\n",
    "                a1, b1, c1 = left_peak\n",
    "                a2, b2, c2 = right_peak\n",
    "\n",
    "                A = 1/c2**2-1/c1**2\n",
    "                B = 2*b1/c1**2-2*b2/c2**2\n",
    "                C = (b2/c2)**2-(b1/c1)**2+np.log(a1/a2)\n",
    "\n",
    "                roots = np.roots([A,B,C])\n",
    "                intersection = np.max(gaussian_fit(roots, a1,b1,c1))\n",
    "                same_spot_value = intersection / short_peak[0]\n",
    "\n",
    "                if same_spot_value <= 0.5:\n",
    "                    layer = 'organized'\n",
    "                    nuclear_peak = left_peak[1]\n",
    "                else:\n",
    "                    layer = 'disorganized'\n",
    "                    nuclear_peak = left_peak[1]\n",
    "            else:\n",
    "                peaks = 1\n",
    "                same_spot_value = 'undef'\n",
    "                if params_single[2] > -0.372*(params_single[1]-min_layer_height) + 3.572:\n",
    "                    layer = 'disorganized'\n",
    "                    nuclear_peak = params_single[1]\n",
    "                else:\n",
    "                    layer = 'organized'\n",
    "                    nuclear_peak = params_single[1]\n",
    "                    \n",
    "            list_of_widths.append(params_single[2])\n",
    "            list_of_peak_positions.append(params_single[1]-min_layer_height)\n",
    "            list_of_peak_numbers.append(peaks)\n",
    "\n",
    "            # Defining needed layer_determination and find_shoulder variables\n",
    "            actin_profile_derivative = smooth_array(np.diff(norm_intensities_actin), window_size = 5)\n",
    "            peaks = signal.find_peaks(actin_profile_derivative, prominence = 0.008)\n",
    "            num_peaks = len(peaks[0])\n",
    "            \n",
    "            if len(peaks[0])==2:\n",
    "                equivalence = peaks[1]['prominences'][1]/peaks[1]['prominences'][0]\n",
    "            else:\n",
    "                equivalence = 'undef'\n",
    "\n",
    "\n",
    "            actin_to_top = max_layer_height - actin_peak\n",
    "            actin_to_bottom = actin_peak - min_layer_height\n",
    "            peak_difference_rule = (actin_peak - nuclear_peak)>= 0\n",
    "            actin_peak_position_rule = (actin_to_top - actin_to_bottom)<0\n",
    "            deriv_peak_rule = num_peaks == 1 \n",
    "\n",
    "            cells_above = np.sum(df_cleared['z']-z_min >= max_layer_height)\n",
    "            cells_inside = np.sum(df_cleared['z']-z_min < max_layer_height)\n",
    "            percentage_above = (cells_above/(cells_above+cells_inside)*100)\n",
    "            cell_density = (cells_inside+cells_above) / (x_max-x_min)**2*1000\n",
    "\n",
    "\n",
    "            if layer == 'disorganized':\n",
    "                layer_classification = 'Disorganized'\n",
    "\n",
    "            else:\n",
    "                if actin_peak_position_rule and peak_difference_rule:\n",
    "                    if deriv_peak_rule or equivalence < 1:\n",
    "                        layer_classification = 'Intermediate' \n",
    "                    else:\n",
    "                        layer_classification = 'Mature'\n",
    "                else:\n",
    "                    layer_classification = 'Immature'\n",
    "\n",
    "            # Defining disorganization_extent variables\n",
    "            actin_channel = kwargs.get('actin_channel', 0)\n",
    "            # We were not able to determine the maximum actin slice/height as before due to the amount of disorganized cells\n",
    "            # above the layer which skew the actin intensity graph such that it's impossible to determine the top of the \n",
    "            # \"layer\" in disorganized cases. In the following line, we use empirical observations to manually determine \n",
    "            # the position of the top of the layer\n",
    "            max_actin_slice_weird = min_actin_slice+15\n",
    "            min_slice = slice_heights[min_actin_slice]\n",
    "            max_slice = slice_heights[min_actin_slice]+8\n",
    "            sum_proj_z = np.sum(image[min_actin_slice:max_actin_slice_weird, actin_channel, :, :], axis = 0)\n",
    "            df_clear_in_layer = df_cleared[df_cleared['z']>(min_slice+z_min)]\n",
    "            df_clear_in_layer = df_clear_in_layer[df_clear_in_layer['z']<(max_slice+z_min)]\n",
    "            real_xs = (df_clear_in_layer['x'] - x_min)*sum_proj_z.shape[0]/(x_max-x_min)\n",
    "            real_ys = (df_clear_in_layer['y'] - y_min)*sum_proj_z.shape[1]/(y_max-y_min)\n",
    "\n",
    "            basal_cells = len(df_clear_in_layer)\n",
    "\n",
    "            list_of_cells_above.append(cells_above)\n",
    "            list_of_percent_above.append(percentage_above)\n",
    "            list_of_cells_in.append(cells_inside)\n",
    "            list_of_layer_class.append(layer_classification)\n",
    "            list_of_cell_densities.append(cell_density)\n",
    "            list_of_layer_heights.append(layer_height)\n",
    "            list_of_disorganization_extent.append(basal_cells)\n",
    "            list_of_equivalences.append(equivalence)\n",
    "\n",
    "            if box_radius != False:\n",
    "                cells_in_box, box_density = local_density(df, image, box_radius = box_radius)\n",
    "                list_of_box_cells.append(cells_in_box)\n",
    "                list_of_box_densities.append(box_density)\n",
    "                \n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            list_of_bad_images.append(i)\n",
    "            continue\n",
    "    list_of_totals = []\n",
    "    for i in range(len(list_of_cells_in)):\n",
    "        list_of_totals.append(list_of_cells_in[i]+list_of_cells_above[i])\n",
    "    if len(list_of_bad_images)>0:\n",
    "        print('The following images would not complete processing: \\n', list_of_bad_images)\n",
    "        dict_list_of_names = np.delete(list_of_names, list_of_bad_images)\n",
    "    else:\n",
    "        dict_list_of_names = list_of_names\n",
    "\n",
    "    if box_radius != False:\n",
    "        dict_data = {'names': dict_list_of_names,\n",
    "                     'Is this a layer?':list_of_layer_class,\n",
    "                     'cells in layer': list_of_cells_in,\n",
    "                     'cells above layer': list_of_cells_above,\n",
    "                     'total number of cells': list_of_totals,\n",
    "                     'disorganized cells in layer' : list_of_disorganization_extent,\n",
    "                     'cell density': list_of_cell_densities,\n",
    "                     'cells in box': list_of_box_cells,\n",
    "                     'box density': list_of_box_densities,\n",
    "                     'layer height': list_of_layer_heights,\n",
    "                     '% above': list_of_percent_above,\n",
    "                     'equivalence': list_of_equivalences,\n",
    "                     'peaks': list_of_peak_numbers,\n",
    "                     'widths': list_of_widths,\n",
    "                     'positions': list_of_peak_positions\n",
    "                    }\n",
    "    else:\n",
    "        dict_data = {'names': dict_list_of_names,\n",
    "                     'Is this a layer?': list_of_layer_class,\n",
    "                     'cells in layer': list_of_cells_in,\n",
    "                     'cells above layer': list_of_cells_above,\n",
    "                     'total number of cells': list_of_totals,\n",
    "                     'disorganized cells in layer' : list_of_disorganization_extent,\n",
    "                     'cell densities': list_of_cell_densities,\n",
    "                     'layer height': list_of_layer_heights,\n",
    "                     '% above': list_of_percent_above,\n",
    "                     'equivalence': list_of_equivalences,\n",
    "                     'peaks': list_of_peak_numbers,\n",
    "                     'widths': list_of_widths,\n",
    "                     'positions': list_of_peak_positions\n",
    "                    }\n",
    "    \n",
    "    analyzed_df = pd.DataFrame(dict_data)\n",
    "    if save_name != False:\n",
    "        analyzed_df.to_csv(save_name + '.csv')\n",
    "        return analyzed_df\n",
    "    else:\n",
    "        return analyzed_df\n",
    "\n",
    "tic = time.time()\n",
    "print(tic-toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This final cell is an example which will output a spreadsheet with the qualitative and quantitative analyses of each image/segmentation input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2029421329498291\n"
     ]
    }
   ],
   "source": [
    "toc = time.time()\n",
    "batch_process(list_of_dfs, list_of_unshuffled_images, list_of_names, save_name = \"name\")\n",
    "tic=time.time()\n",
    "print(tic-toc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
